{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing Jupyter notebook from data_cleanup.ipynb\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "pd.set_option('display.max_columns', None)  # Unlimited columns\n",
    "import nbimporter\n",
    "# Imported from my other notebook\n",
    "from data_cleanup import cleanup1\n",
    "from data_cleanup import cleanup2\n",
    "from data_cleanup import cleanup3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all data\n",
    "sample_submission = pd.read_csv('original_data/sample_submission.csv')\n",
    "test_features = pd.read_csv('original_data/test_features.csv')\n",
    "train_features = pd.read_csv('original_data/train_features.csv')\n",
    "train_labels = pd.read_csv('original_data/train_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean data with previously defined cleanup function\n",
    "train1 = cleanup(train_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline prediction\n",
    "Always start with a stupid model, no exceptions.  In this case, the stupid model is assuming the majority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict that all rows belong to the majority class\n",
    "majority_class = train_labels['status_group'].mode()[0]\n",
    "y_pred = np.full(len(train_labels), majority_class)\n",
    "y_true = train_labels['status_group']\n",
    "\n",
    "# Check the accuracy of that prediction\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OHE + Logistic Regression\n",
    "I'll start by one-hot encoding all the categorical variables and running a simple logistic regression.  Many of the features have way too much cardinality for one-hot encoding.  Let's separate them into two lists by cardinality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort all features by cardinality.\n",
    "train1.select_dtypes(exclude='number').nunique().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'll first drop the categorical variables that have too many \n",
    "# unique values, so that regression doesn't take forever\n",
    "cols_to_drop = ['id',\n",
    "                 'wpt_name',\n",
    "                 'subvillage',\n",
    "                 'scheme_name',\n",
    "                 'installer',\n",
    "                 'ward',\n",
    "                 'funder',\n",
    "                 ]\n",
    "\n",
    "\n",
    "cols_to_keep =  ['lga',\n",
    "                 'region_code',\n",
    "                 'region',\n",
    "                 'district_code',\n",
    "                 'extraction_type_group',\n",
    "                 'management',\n",
    "                 'source',\n",
    "                 'scheme_management',\n",
    "                 'extraction_type',\n",
    "                 'basin',\n",
    "                 'water_quality',\n",
    "                 'payment_type',\n",
    "                 'extraction_type_class',\n",
    "                 'waterpoint_type',\n",
    "                 'source_type',\n",
    "                 'payment',\n",
    "                 'waterpoint_type_group',\n",
    "                 'quality_group',\n",
    "                 'quantity',\n",
    "                 'quantity_group',\n",
    "                 'management_group',\n",
    "                 'public_meeting',\n",
    "                 'permit',\n",
    "                 'source_class']\n",
    "# X = train1.drop(columns= )\n",
    "X = train1[cols_to_keep]\n",
    "y_true = train_labels['status_group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "\n",
    "# Get a list of numerical and categorical columns\n",
    "numerical_cols = X.select_dtypes(include='number').columns.tolist()\n",
    "categorical_cols = X.select_dtypes(exclude='number').columns.tolist()\n",
    "\n",
    "# # Use a mapper to apply transformations selectively\n",
    "mapper = DataFrameMapper(\n",
    "  [([col], StandardScaler()) for col in numerical_cols] +\n",
    "  [([col], OneHotEncoder(categories='auto')) for col in categorical_cols]\n",
    ")\n",
    "\n",
    "# # Define an estimator and param_grid\n",
    "pipe = make_pipeline(\n",
    "    mapper, \n",
    "    LogisticRegression(solver='lbfgs', multi_class='ovr',\n",
    "                      max_iter=500))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "pipe.fit(X,y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipe.predict(X)\n",
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, that was the score with all categories except for those that have thousands of possible values and make the final dataframe way too big."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the test dataset\n",
    "test1 = cleanup1(test_features)\n",
    "\n",
    "# Extract the same columns used for training\n",
    "X_test = test1[cols_to_keep]\n",
    "\n",
    "# Run the prediction, using the pipeline fit to the training data\n",
    "y_pred = pipe.predict(X_test)\n",
    "\n",
    "# Make a dataframe with the answers\n",
    "y_submit = pd.DataFrame({'id':test_features['id'],\n",
    "                         'status_group':y_pred} )\n",
    "# make a submission CSV file\n",
    "y_submit.to_csv('DMAn.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All I need for restarted kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "pd.set_option('display.max_columns', None)  # Unlimited columns\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('original_data/sample_submission.csv')\n",
    "test_features = pd.read_csv('original_data/test_features.csv')\n",
    "train_features = pd.read_csv('original_data/train_features.csv')\n",
    "train_labels = pd.read_csv('original_data/train_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup1(X):\n",
    "    \"\"\"\n",
    "    Ensures that all the features are good to go for the first \n",
    "    logistic regression.\n",
    "    \n",
    "    >> Input\n",
    "    X: Full-featured dataset\n",
    "    \n",
    "    >> Output\n",
    "    X2: Cleaned dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    # Looking at all the features with missing values, it looks like those\n",
    "    # features are all categorical variables where 'unknown' would be a\n",
    "    # category we can work with.  I'll replace the NANs accordingly.\n",
    "    X2 = X.fillna('unknown')\n",
    "    \n",
    "    # Regression on dates won't work.  Instead, I'll turn the \n",
    "    # date_recorded column into the number of years since 2000\n",
    "    # (the earliest date in the training date is from 2000, and the\n",
    "    # latest from 2013.)\n",
    "    dates = pd.to_datetime(X2.date_recorded)\n",
    "    year2000 = pd.to_datetime('2000-01-01')\n",
    "    years = [i.days/365 for i in (dates - year2000)]\n",
    "    X2.date_recorded = years\n",
    "    \n",
    "    # region_code and district_code are int64, but they should really be\n",
    "    # treated as categories (and there's only 20-30 classes in each).\n",
    "    # I'll cast them as strings instead.\n",
    "    X2.region_code = X2.region_code.astype('str')\n",
    "    X2.district_code = X2.district_code.astype('str')\n",
    "    \n",
    "    # recorded_by has only one value everywhere, and is therefore useless\n",
    "    X2 = X2.drop(columns='recorded_by')\n",
    "    \n",
    "    # To prevent data conversion warnings, I'll turn all the numerical\n",
    "    # features (except id) into float64.\n",
    "    \n",
    "    # Also, some columns contained bool values and NANs.  \n",
    "    # (e.g., public_meeting, permit)\n",
    "    # I replaced the NANs with strings, so I'll cast the whole series \n",
    "    # as strings to prevent future problems with data type heterogeneity.\n",
    "    type_dict = {'amount_tsh':'float64',\n",
    "                 'date_recorded':'float64',\n",
    "                 'gps_height':'float64',\n",
    "                 'longitude':'float64',\n",
    "                 'latitude':'float64',\n",
    "                 'num_private':'float64',\n",
    "                 'population':'float64',\n",
    "                 'construction_year':'float64',\n",
    "                 'public_meeting':'str',\n",
    "                 'permit':'str'}\n",
    "    \n",
    "    X2 = X2.astype(dtype = type_dict)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    return X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1 = cleanup1(train_features)\n",
    "train1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LogReg with OHE and Binary Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.decomposition import PCA\n",
    "from category_encoders import BinaryEncoder\n",
    "\n",
    "bad_cats = ['wpt_name',\n",
    "                 'subvillage',\n",
    "                 'scheme_name',\n",
    "                 'installer',\n",
    "                 'ward',\n",
    "                 'funder',\n",
    "                 ]\n",
    "\n",
    "\n",
    "good_cats =  ['lga',\n",
    "                 'region_code',\n",
    "                 'region',\n",
    "                 'district_code',\n",
    "                 'extraction_type_group',\n",
    "                 'management',\n",
    "                 'source',\n",
    "                 'scheme_management',\n",
    "                 'extraction_type',\n",
    "                 'basin',\n",
    "                 'water_quality',\n",
    "                 'payment_type',\n",
    "                 'extraction_type_class',\n",
    "                 'waterpoint_type',\n",
    "                 'source_type',\n",
    "                 'payment',\n",
    "                 'waterpoint_type_group',\n",
    "                 'quality_group',\n",
    "                 'quantity',\n",
    "                 'quantity_group',\n",
    "                 'management_group',\n",
    "                 'public_meeting',\n",
    "                 'permit',\n",
    "                 'source_class']\n",
    "\n",
    "X = train1.drop(columns='id')\n",
    "y_true = train_labels['status_group']\n",
    "\n",
    "# Get a list of numerical columns\n",
    "numerical_cols = X.select_dtypes(include='number').columns.tolist()\n",
    "\n",
    "# # Use a mapper to apply transformations selectively\n",
    "mapper = DataFrameMapper(\n",
    "    [([col], StandardScaler()) for col in numerical_cols] +\n",
    "    [([col], OneHotEncoder(categories='auto')) for col in good_cats] +\n",
    "    [([col], BinaryEncoder()) for col in bad_cats]   \n",
    ")\n",
    "\n",
    "# # Define an estimator and param_grid\n",
    "pipe1 = make_pipeline(\n",
    "    mapper,\n",
    "    PCA(n_components=0.99)\n",
    ")\n",
    "\n",
    "pipe2 = make_pipeline(\n",
    "    LogisticRegression(solver = 'lbfgs', multi_class='ovr',\n",
    "                      max_iter=500))\n",
    "\n",
    "param_grid = {}\n",
    "\n",
    "gs = GridSearchCV(pipe2, cv=2, param_grid=param_grid,\n",
    "                  scoring='accuracy', \n",
    "                  verbose=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "X_transformed = pipe1.fit_transform(X,y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pipe2.fit(X_transformed,y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipe2.predict(X_transformed)\n",
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding polynomial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.decomposition import PCA\n",
    "from category_encoders import BinaryEncoder\n",
    "\n",
    "bad_cats = ['wpt_name',\n",
    "                 'subvillage',\n",
    "                 'scheme_name',\n",
    "                 'installer',\n",
    "                 'ward',\n",
    "                 'funder',\n",
    "                 ]\n",
    "\n",
    "\n",
    "good_cats =  ['lga',\n",
    "                 'region_code',\n",
    "                 'region',\n",
    "                 'district_code',\n",
    "                 'extraction_type_group',\n",
    "                 'management',\n",
    "                 'source',\n",
    "                 'scheme_management',\n",
    "                 'extraction_type',\n",
    "                 'basin',\n",
    "                 'water_quality',\n",
    "                 'payment_type',\n",
    "                 'extraction_type_class',\n",
    "                 'waterpoint_type',\n",
    "                 'source_type',\n",
    "                 'payment',\n",
    "                 'waterpoint_type_group',\n",
    "                 'quality_group',\n",
    "                 'quantity',\n",
    "                 'quantity_group',\n",
    "                 'management_group',\n",
    "                 'public_meeting',\n",
    "                 'permit',\n",
    "                 'source_class']\n",
    "\n",
    "X = train1.drop(columns='id')\n",
    "y_true = train_labels['status_group']\n",
    "\n",
    "# Get a list of numerical columns\n",
    "numerical_cols = X.select_dtypes(include='number').columns.tolist()\n",
    "\n",
    "# # Use a mapper to apply transformations selectively\n",
    "scaler_encoder = DataFrameMapper(\n",
    "    [([col], StandardScaler()) for col in numerical_cols] +\n",
    "    [([col], OneHotEncoder(categories='auto')) for col in good_cats] +\n",
    "    [([col], BinaryEncoder()) for col in bad_cats]     \n",
    ")\n",
    "\n",
    "\n",
    "poly_maker = DataFrameMapper(\n",
    "    [([col], PolynomialFeatures()) for col in (numerical_cols + )]    \n",
    ")\n",
    "\n",
    "# # Define an estimator and param_grid\n",
    "pipe1 = make_pipeline(\n",
    "    scaler_encoder,\n",
    "    PCA(n_components=0.99)\n",
    ")\n",
    "\n",
    "pipe2 = make_pipeline(\n",
    "    LogisticRegression(solver = 'lbfgs', multi_class='ovr',\n",
    "                      max_iter=500))\n",
    "\n",
    "param_grid = {}\n",
    "\n",
    "gs = GridSearchCV(pipe2, cv=2, param_grid=param_grid,\n",
    "                  scoring='accuracy', \n",
    "                  verbose=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_expanded = mapper.fit_transform(X,y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_expanded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_expanded, columns=mapper.transformed_names_).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "X_transformed = pipe1.fit_transform(X,y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pipe2.fit(X_transformed,y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipe2.predict(X_transformed)\n",
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transformed.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
