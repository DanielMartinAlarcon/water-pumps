{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "pd.set_option('display.max_columns', None)  # Unlimited columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('original_data/sample_submission.csv')\n",
    "test_features = pd.read_csv('original_data/test_features.csv')\n",
    "train_features = pd.read_csv('original_data/train_features.csv')\n",
    "train_labels = pd.read_csv('original_data/train_labels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleanup\n",
    "I don't want to start out by using all the features available, as several of them are probably useless and require lots of cleanup.  So I'll create several cleanup functions that extract different subsets of the features.  I write them as functions so that I can easily apply them to the training and testing data equally.  This way, also, I don't need to commit to a particular set of features ahead of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def cleanup1(X):\n",
    "    \"\"\"\n",
    "    Minimal viable cleaning.\n",
    "    \n",
    "    This function gets the data in minimal working order for a logistic \n",
    "    regression. I fill up NANs (which appear only in the categorcial\n",
    "    features), change datetime objects to numbers, drop one useless \n",
    "    feature and standardize the datatypes.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : pandas.DataFrame (DF)\n",
    "        Original, full-featured DF (train_features or test_features)\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    X2 : pandas.DataFrame\n",
    "        Cleaned DF\n",
    "    \"\"\"\n",
    " \n",
    "    # Make a clean copy, to ensure we're not changing the original DF\n",
    "    X2 = X.copy()\n",
    "    \n",
    "    # Looking at all the features with missing values, it looks like those\n",
    "    # features are all categorical variables where 'unknown' would be a\n",
    "    # category we can work with.  I'll replace the NANs accordingly.\n",
    "    X2 = X2.fillna('unknown')\n",
    "    \n",
    "    # Regression on dates won't work.  Instead, I'll turn the \n",
    "    # date_recorded column into the number of years since 2000\n",
    "    # (the earliest date in the training date is from ~2002, and the\n",
    "    # latest from 2013.)\n",
    "    dates = pd.to_datetime(X2.date_recorded)\n",
    "    year2000 = pd.to_datetime('2000-01-01')\n",
    "    years = [i.days/365 for i in (dates - year2000)]\n",
    "    X2.date_recorded = years\n",
    "    \n",
    "    # region_code and district_code are int64, but they should really be\n",
    "    # treated as categories (and there's only 20-30 classes in each).\n",
    "    # I'll cast them as strings instead.\n",
    "    X2.region_code = X2.region_code.astype('str')\n",
    "    X2.district_code = X2.district_code.astype('str')\n",
    "    \n",
    "    # recorded_by has only one value everywhere, and is therefore useless\n",
    "    X2 = X2.drop(columns='recorded_by')\n",
    "    \n",
    "    # To prevent data conversion warnings, I'll turn all the numerical\n",
    "    # features (except id) into float64.\n",
    "    \n",
    "    # Also, some columns contained bool values and NANs.  \n",
    "    # (e.g., public_meeting, permit)\n",
    "    # I replaced the NANs with strings, which created a problem for later\n",
    "    # operations that don't like heterogeneous datatypes within a single\n",
    "    # column. I'll prevent this problem by casting those two features as str.\n",
    "    \n",
    "    type_dict = {'amount_tsh':'float64',\n",
    "                 'date_recorded':'float64',\n",
    "                 'gps_height':'float64',\n",
    "                 'longitude':'float64',\n",
    "                 'latitude':'float64',\n",
    "                 'num_private':'float64',\n",
    "                 'population':'float64',\n",
    "                 'construction_year':'float64',\n",
    "                 'public_meeting':'str',\n",
    "                 'permit':'str'}\n",
    "    \n",
    "    X2 = X2.astype(dtype = type_dict)\n",
    "    \n",
    "    return X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import MissingIndicator\n",
    "\n",
    "def cleanup2(X):\n",
    "    \"\"\"\n",
    "    Fixes the numerical features. \n",
    "    \n",
    "    \n",
    "    Each feature has different specific problems, but they usually have\n",
    "    garbage values (usually zero) that should really be read as NANs.\n",
    "    \n",
    "    I want to fix those values, but I also want to take note of the \n",
    "    datapoints where they happened.  I do this because I assume that \n",
    "    missing values tell us something about the well that our model\n",
    "    might be able to pick up later.\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : pandas.DataFrame\n",
    "        DF with raw numerical features\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    X2 : pandas.DataFrame\n",
    "         DF with cleaned numerical features and a new matrix of former\n",
    "         garbage locations within those features.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Make a clean copy, to ensure we're not changing the original DF\n",
    "    X2 = X.copy()\n",
    "    \n",
    "    # I make a list of the numerical columns and a dict of their \n",
    "    # garbage values that really should be nulls\n",
    "    numericals = ['amount_tsh',\n",
    "                    'date_recorded',\n",
    "                    'gps_height',\n",
    "                    'longitude',\n",
    "                    'latitude',\n",
    "                    'num_private',\n",
    "                    'population',\n",
    "                    'construction_year']\n",
    "\n",
    "    null_values = {'amount_tsh':0,\n",
    "                     'date_recorded':0,\n",
    "                     'gps_height':0,\n",
    "                     'longitude':0,\n",
    "                     'latitude':-2.000000e-08,\n",
    "                     'num_private':0,\n",
    "                     'population':0,\n",
    "                     'construction_year':0}\n",
    "\n",
    "    # I replace all garbage values with NANs.\n",
    "    for feature, null in null_values.items():\n",
    "        X2[feature] = X2[feature].replace(null, np.nan)\n",
    "\n",
    "    # construction_year occasionally claims years far in the future, and \n",
    "    # could presumably also contain years way in the past.  I'll turn anything\n",
    "    # not between 1960 and 2019 into a NAN.\n",
    "    X2['construction_year'] = [i if 1960 < i < 2019 else np.nan for i in X2['construction_year']]\n",
    "    \n",
    "    \n",
    "    # Creating indicator columns.\n",
    "    # ---------------------------------------------------------------\n",
    "    # These columns mark the locations of all the NANs \n",
    "    # in the numericals. Note that MissingIndicator returns a numpy array.\n",
    "    \n",
    "    indicator = MissingIndicator()\n",
    "    trash_array = indicator.fit_transform(X2[numericals]) # Bool array\n",
    "    trash_array = trash_array.astype('float64')     # Float64 array\n",
    "\n",
    "    # Create a titles for the columns in num_trashmarker\n",
    "    trashy_names = [numericals[i] + '_trash' for i in indicator.features_]\n",
    "\n",
    "    # Create num_trashmarker\n",
    "    trash_df = pd.DataFrame(trash_array, columns=trashy_names)\n",
    "\n",
    "    # I add trash_df to X2\n",
    "    X2 = pd.concat([X2,trash_df], sort=False, axis=1)\n",
    "    \n",
    "    \n",
    "    # Fixing the numerical columns.\n",
    "    # ---------------------------------------------------------------\n",
    "    # Whenever possible, a good replacement value for a NAN is the \n",
    "    # mean or median value for the geographic region around it.\n",
    "\n",
    "    # Replaces the NANs in a ward with the mean of the other rows in that \n",
    "    # same ward. If all the rows in a ward are NANs, though, they remain.\n",
    "    for feature in numericals:\n",
    "        replacements = X2.groupby('ward')[feature].transform('mean')\n",
    "        X2[feature] = X2[feature].fillna(replacements)\n",
    "\n",
    "    # Replaces the NANs in a region with the mean of the other rows in that \n",
    "    # same region (which are much larger than wards)\n",
    "    for feature in numericals:\n",
    "        replacements = X2.groupby('region')[feature].transform('mean')\n",
    "        X2[feature] = X2[feature].fillna(replacements)\n",
    "    \n",
    "    # Replaces any remaining NANs with the median value for the whole dataset\n",
    "    for feature in numericals:\n",
    "        replacements = X2[feature].median() # Single number, not array\n",
    "        X2[feature] = X2[feature].fillna(replacements)\n",
    "    \n",
    "    return X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def cleanup3(X):\n",
    "    \"\"\"\n",
    "    Fixes the categorical features. \n",
    "    \n",
    "    \n",
    "    Each feature has different specific problems, but they usually have\n",
    "    garbage values (usually 'unknown') that should really be read as NANs.\n",
    "    \n",
    "    This function cleans up garbage, clusters together different labels\n",
    "    that should be equivalent but are coded differently (e.g., different\n",
    "    spellings of the same thing), and removes labels with so few members\n",
    "    that they're unlikely to be informative.\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : pandas.DataFrame\n",
    "        DF with raw categorical features, except for the changes\n",
    "        already included in cleanup1.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    X2 : pandas.DataFrame\n",
    "         DF with cleaned categorical features.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Make a clean copy, to ensure we're not changing the original DF\n",
    "    X2 = X.copy()\n",
    "    \n",
    "    # Create list of categorical features\n",
    "    categoricals = X2.select_dtypes(exclude='number').columns.tolist()\n",
    "\n",
    "    # Make all strings lowercase, to collapse together some of the categories\n",
    "    X2[categoricals] = X2[categoricals].applymap(lambda x: x.lower())\n",
    "\n",
    "    # Replace common NAN values\n",
    "    nan_list = ['not known','unknown','none','-','##','not kno','unknown installer']\n",
    "    X2 = X2.replace(nan_list, np.nan)\n",
    "\n",
    "    # Any feature values with fewer than 100 rows gets turned into a NAN\n",
    "    for feature in X2[categoricals]:\n",
    "        # Determine which feature values to keep\n",
    "        to_keep = X2[feature].value_counts()[X2[feature].value_counts() > 100].index.tolist()\n",
    "        # Turn those into NANs (using a copy, to prevent warnings)\n",
    "        feature_copy = X2[feature].copy()\n",
    "        feature_copy[~feature_copy.isin(to_keep)] = np.nan\n",
    "        X2[feature] = feature_copy\n",
    "\n",
    "    # Fix all NANs\n",
    "    X2[categoricals] = X2[categoricals].fillna('other')\n",
    "    \n",
    "    \n",
    "    return X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "def cleanup4(X):\n",
    "    \"\"\"\n",
    "    Gets rid of mostly useless features, adds a couple of engineered ones,\n",
    "    and standardizes the numericals. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : pandas.DataFrame\n",
    "        DF cleaned with cleanup 1-3\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    X2 : pandas.DataFrame\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Make a clean copy, to ensure we're not changing the original DF\n",
    "    X2 = X.copy()\n",
    "    \n",
    "    garbage = ['longitude','latitude','construction_year_trash',\n",
    "              'latitude_trash','gps_height_trash',\n",
    "               'extraction_type_group','extraction_type_class',\n",
    "               'region_code','waterpoint_type_group','source_type',\n",
    "              'payment_type','quality_group','quantity_group']\n",
    "    \n",
    "    X2 = X2.drop(columns=garbage)\n",
    "    \n",
    "    X2['age'] = X2['date_recorded'] - X2['construction_year']\n",
    "\n",
    "    numericals = ['amount_tsh',\n",
    "                    'date_recorded',\n",
    "                    'gps_height',\n",
    "                    'num_private',\n",
    "                    'population',\n",
    "                    'construction_year',\n",
    "                    'age']\n",
    "\n",
    "    scaler = RobustScaler()\n",
    "    nums_scaled = scaler.fit_transform(X2[numericals])\n",
    "    nums_scaled = pd.DataFrame(nums_scaled, columns=numericals)\n",
    "    X2[numericals] = nums_scaled\n",
    "    \n",
    "    return X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from category_encoders.target_encoder import TargetEncoder\n",
    "def target_encode_cats(X, X_train, cats, train_labels):\n",
    "    \"\"\"\n",
    "    Target encodes a DF of categorical features, based on the three\n",
    "    component vectors of y_true.  Target encoding is designed to work with\n",
    "    binary labels; in order to make it work with a vector that has three\n",
    "    values, I target encode against a binary version of each and then\n",
    "    concatenate the results.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : pandas.DataFrame\n",
    "        Dataset to be fixed\n",
    "        \n",
    "    cats : List of categorical columns to encode\n",
    "\n",
    "    train_labels : pandas.DataFrame\n",
    "                    The vector of training labels\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    X2 : pandas.DataFrame\n",
    "            Fixed vector\n",
    "\n",
    "    \"\"\"\n",
    "    # Make a clean copy, to ensure we're not changing the original DF\n",
    "    X2 = X.copy()\n",
    "    \n",
    "    y_true = train_labels['status_group']\n",
    "    y_works = [1.0 if x == 'functional' else 0.0 for x in y_true]\n",
    "    y_broken = [1.0 if x == 'non functional' else 0.0 for x in y_true]\n",
    "    y_repair = [1.0 if x == 'functional needs repair' else 0.0 for x in y_true]\n",
    "\n",
    "    y_vectors = [y_works, y_broken, y_repair]\n",
    "    X_TE_all = []\n",
    "\n",
    "    # We want to create encoding based on the training features and \n",
    "    # labels, but apply this encoding to any vector (such as X_test)\n",
    "    for i in [1,2,3]:\n",
    "        # Make an encoder\n",
    "        TE = TargetEncoder()\n",
    "        \n",
    "        # Fit it to the training data\n",
    "        TE.fit(X=X_train[cats], y=y_vectors[i-1])\n",
    "\n",
    "        # Transform the cat columns in X\n",
    "        X_TE = TE.transform(X2[cats])\n",
    "        \n",
    "        # Give them custom names, so that the columns encoded against\n",
    "        # each target vector have a different name\n",
    "        X_TE = X_TE.rename(columns=(lambda x: x + '_TE' + str(i)))\n",
    "        X_TE_all.append(X_TE)\n",
    "\n",
    "    new_cats = pd.concat(X_TE_all, sort=False, axis=1)\n",
    "    \n",
    "    X2 = X2.drop(columns=cats)\n",
    "    X2 = pd.concat([X2,new_cats], sort=False, axis=1)\n",
    "    \n",
    "    return X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "categoricals = ['funder',\n",
    "                     'installer',\n",
    "                     'wpt_name',\n",
    "                     'basin',\n",
    "                     'subvillage',\n",
    "                     'region',\n",
    "                     'district_code',\n",
    "                     'lga',\n",
    "                     'ward',\n",
    "                     'public_meeting',\n",
    "                     'scheme_management',\n",
    "                     'scheme_name',\n",
    "                     'permit',\n",
    "                     'extraction_type',\n",
    "                     'management',\n",
    "                     'management_group',\n",
    "                     'payment',\n",
    "                     'water_quality',\n",
    "                     'quantity',\n",
    "                     'source',\n",
    "                     'source_class',\n",
    "                     'waterpoint_type',]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing cleanup steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_temp = cleanup4(cleanup3(cleanup2(cleanup1(train_features))))\n",
    "X_train_new = target_encode_cats(X=X_train_temp, \n",
    "                                 X_train=X_train_temp, \n",
    "                                 cats=categoricals, \n",
    "                                 train_labels=train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 36s, sys: 662 ms, total: 1min 37s\n",
      "Wall time: 1min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from xgboost import XGBClassifier\n",
    "modelxgb = XGBClassifier(objective = 'multi:softmax', booster = 'gbtree', nrounds = 'min.error.idx', \n",
    "                      num_class = 3, maximize = False, eval_metric = 'merror', eta = .1,\n",
    "                      max_depth = 14, colsample_bytree = .4)\n",
    "\n",
    "y_true = train_labels['status_group']\n",
    "modelxgb.fit(X_train_new, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9288215488215488"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test on training data\n",
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = modelxgb.predict(X_train_new)\n",
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_temp = cleanup4(cleanup3(cleanup2(cleanup1(test_features))))\n",
    "X_test_new = target_encode_cats(X=X_test_temp, \n",
    "                                X_train=X_train_temp, \n",
    "                                cats=categoricals, \n",
    "                                train_labels=train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict test data from the model\n",
    "y_test_pred = modelxgb.predict(X_test_new)\n",
    "\n",
    "# Make a dataframe with the answers\n",
    "y_submit = pd.DataFrame({'id':test_features['id'],\n",
    "                         'status_group':y_test_pred} )\n",
    "# make a submission CSV file\n",
    "y_submit.to_csv('DMAn.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
