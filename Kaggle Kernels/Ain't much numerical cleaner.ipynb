{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('original_data/sample_submission.csv')\n",
    "test_features = pd.read_csv('original_data/test_features.csv')\n",
    "train_features = pd.read_csv('original_data/train_features.csv')\n",
    "train_labels = pd.read_csv('original_data/train_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup1(X):\n",
    "    \"\"\"\n",
    "    Minimal viable cleaning.\n",
    "    \n",
    "    This function gets the data in minimal working order for a logistic \n",
    "    regression. I fill up NANs (which appear only in the categorcial\n",
    "    features), change datetime objects to numbers, drop one useless \n",
    "    feature and standardize the datatypes.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : pandas.DataFrame (DF)\n",
    "        Original, full-featured DF (train_features or test_features)\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    X2 : pandas.DataFrame\n",
    "        Cleaned DF\n",
    "    \"\"\"\n",
    " \n",
    "    # Make a clean copy, to ensure we're not changing the original DF\n",
    "    X2 = X.copy()\n",
    "    \n",
    "    # Looking at all the features with missing values, it looks like those\n",
    "    # features are all categorical variables where 'unknown' would be a\n",
    "    # category we can work with.  I'll replace the NANs accordingly.\n",
    "    X2 = X2.fillna('unknown')\n",
    "    \n",
    "    # Regression on dates won't work.  Instead, I'll turn the \n",
    "    # date_recorded column into the number of years since 2000\n",
    "    # (the earliest date in the training date is from ~2002, and the\n",
    "    # latest from 2013.)\n",
    "    dates = pd.to_datetime(X2.date_recorded)\n",
    "    year2000 = pd.to_datetime('2000-01-01')\n",
    "    years = [i.days/365 for i in (dates - year2000)]\n",
    "    X2.date_recorded = years\n",
    "    \n",
    "    # region_code and district_code are int64, but they should really be\n",
    "    # treated as categories (and there's only 20-30 classes in each).\n",
    "    # I'll cast them as strings instead.\n",
    "    X2.region_code = X2.region_code.astype('str')\n",
    "    X2.district_code = X2.district_code.astype('str')\n",
    "    \n",
    "    # recorded_by has only one value everywhere, and is therefore useless\n",
    "    X2 = X2.drop(columns='recorded_by')\n",
    "    \n",
    "    # To prevent data conversion warnings, I'll turn all the numerical\n",
    "    # features (except id) into float64.\n",
    "    \n",
    "    # Also, some columns contained bool values and NANs.  \n",
    "    # (e.g., public_meeting, permit)\n",
    "    # I replaced the NANs with strings, which created a problem for later\n",
    "    # operations that don't like heterogeneous datatypes within a single\n",
    "    # column. I'll prevent this problem by casting those two features as str.\n",
    "    \n",
    "    type_dict = {'amount_tsh':'float64',\n",
    "                 'date_recorded':'float64',\n",
    "                 'gps_height':'float64',\n",
    "                 'longitude':'float64',\n",
    "                 'latitude':'float64',\n",
    "                 'num_private':'float64',\n",
    "                 'population':'float64',\n",
    "                 'construction_year':'float64',\n",
    "                 'public_meeting':'str',\n",
    "                 'permit':'str'}\n",
    "    \n",
    "    X2 = X2.astype(dtype = type_dict)\n",
    "    \n",
    "    return X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import MissingIndicator\n",
    "\n",
    "def cleanup2(X):\n",
    "    \"\"\"\n",
    "    Fixes the numerical features. \n",
    "    \n",
    "    \n",
    "    Each feature has different specific problems, but they usually have\n",
    "    garbage values (usually zero) that should really be read as NANs.\n",
    "    \n",
    "    I want to fix those values, but I also want to take note of the \n",
    "    datapoints where they happened.  I do this because I assume that \n",
    "    missing values tell us something about the well that our model\n",
    "    might be able to pick up later.\n",
    "    \n",
    "    This function removes the numerical features from the dataset and \n",
    "    makes two copies of them: num_fixed and num_nulls. In null_fixed,\n",
    "    I will replace the garbage values with something better (usually the\n",
    "    mean for that whole feature). In num_nulls, I will replace the \n",
    "    garbage values with NANs.  \n",
    "    \n",
    "    I'll then use MissingIndicator to turn num_nulls into a DF \n",
    "    containing a 1 at each location where a NAN was found. This DF\n",
    "    will be called num_trashmarker\n",
    "    \n",
    "    I'll then go back to the original dataset, and add in both \n",
    "    null_fixed and num_indicator where the original numerical columns\n",
    "    used to be.\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : pandas.DataFrame\n",
    "        DF with raw numerical features\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    X2 : pandas.DataFrame\n",
    "         DF with cleaned numerical features and a new matrix of former\n",
    "         garbage locations within those features.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Make a clean copy, to ensure we're not changing the original DF\n",
    "    X2 = X.copy()\n",
    "    \n",
    "    # Make a list of numerical column names\n",
    "    num_names = X2.select_dtypes('number').columns.tolist()\n",
    "    \n",
    "    # Make two copies of the numerical columns\n",
    "    num_fixed = X2[num_names].copy()\n",
    "    num_nulls = X2[num_names].copy()\n",
    "    \n",
    "    # ---------------------------------------------------------------\n",
    "    # For each numerical feature, fix it in num_fixed and add a NAN\n",
    "    # in num_nulls.  Note how, when I use a feature's mean as the \n",
    "    # fill-in value, I calculate that mean on the feature where\n",
    "    # garbage values have already been turned into NANs, so that \n",
    "    # the garbage doesn't affect the mean.\n",
    "    \n",
    "    # Longitudes of 0 are trash\n",
    "    i = 'longitude'\n",
    "    trash = 0\n",
    "    num_nulls[i] = num_nulls[i].replace(trash, np.nan)\n",
    "    mean = num_nulls[i].mean()\n",
    "    num_fixed[i] = num_fixed[i].replace(trash, mean)\n",
    "    \n",
    "    # Latitudes of -2.000000e-08 are trash\n",
    "    i = 'latitude'\n",
    "    trash = -2.000000e-08\n",
    "    num_nulls[i] = num_nulls[i].replace(trash, np.nan)\n",
    "    mean = num_nulls[i].mean()\n",
    "    num_fixed[i] = num_fixed[i].replace(trash, mean)\n",
    "    \n",
    "    # I don't know what num_private is supposed to mean, but there sure are\n",
    "    # a lot of zero values.  Those tend to be garbage in other features, so\n",
    "    # I'll mark them as if they were NANs in num_nulls just in case that \n",
    "    # means something.  I won't change them to something else, though.\n",
    "    i = 'num_private'\n",
    "    trash = 0\n",
    "    num_nulls[i] = num_nulls[i].replace(trash, np.nan)\n",
    "\n",
    "    # I bet that population=0 could be a legitimate value, but it's also \n",
    "    # susupicious. I'll mark those rows as if they were NANs in num_nulls \n",
    "    # just in case, but I won't change them to something else.\n",
    "    i = 'population'\n",
    "    trash = 0\n",
    "    num_nulls[i] = num_nulls[i].replace(trash, np.nan)\n",
    "    \n",
    "    # construction_year values of zero are garbage.  I'll replace them with \n",
    "    # the earliest year in the list (1960) and change all the values to the\n",
    "    # number of years since then. I bet that assuming these pumps are older\n",
    "    # is better than assuming they are of average age.\n",
    "    i = 'construction_year'\n",
    "    trash = 0\n",
    "    num_nulls[i] = num_nulls[i].replace(trash, np.nan)\n",
    "    num_fixed[i] = num_fixed[i].replace(0.0, 1960)\n",
    "    num_fixed[i] = num_fixed[i] - 1960.0\n",
    "       \n",
    "    # --------------------------------------------------------------- \n",
    "    # Create indicator columns that mark the locations of all the NANs \n",
    "    # in the numerical columns, and add back to the full DF. Note\n",
    "    # that MissingIndicator returns a numpy array.\n",
    "\n",
    "    indicator = MissingIndicator()\n",
    "    trash_array = indicator.fit_transform(num_nulls) # Bool array\n",
    "    trash_array = trash_array.astype('float64')     # Float64 array\n",
    "    \n",
    "    # Create a titles for the columns in num_trashmarker\n",
    "    num_names_trashy = [num_names[i] + '_trash' for i in indicator.features_]\n",
    "    \n",
    "    # Create num_trashmarker\n",
    "    num_trashmarker = pd.DataFrame(trash_array, columns=num_names_trashy)\n",
    "    \n",
    "    # Drop the numerical columns from X2, replace them with the fixed \n",
    "    # ones, and add the trash markers.\n",
    "    X2 = X2.drop(columns=num_names)\n",
    "    X2 = pd.concat([num_fixed, num_trashmarker, X2], sort=False, axis=1)\n",
    "    \n",
    "    return X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train is the matrix of features that will go into the logistic regression.\n",
    "# It exists at various points as a dataframe or numpy array\n",
    "X_train = cleanup2(cleanup1(train_features))\n",
    "y_train = train_labels['status_group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This command produces a series of the categorical features, calculates their cardinality\n",
    "# (number of unique values), sorts the features by cardinality, extracts the feature names\n",
    "# (indices), turns those indexes into a list, and takes all but the 6 with highest cardinality. \n",
    "cols_to_keep = X_train.select_dtypes(exclude='number').nunique().sort_values().index.tolist()[:-6]\n",
    "cols_to_keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = X_train.drop(columns=cols_to_drop)\n",
    "X_train = X_train[cols_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this cell I define a pipeline that will scale and one-hot encode X_train, then\n",
    "# feed it to the logistic regression.\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "\n",
    "# Get lists of numerical and categorical features\n",
    "numerical_cols = X_train.select_dtypes(include='number').columns.tolist()\n",
    "categorical_cols = X_train.select_dtypes(exclude='number').columns.tolist()\n",
    "\n",
    "# Use a mapper to apply different transformations to the numerical and\n",
    "# categorical features\n",
    "mapper = DataFrameMapper(\n",
    "  [([col], RobustScaler()) for col in numerical_cols] +\n",
    "  [([col], OneHotEncoder(categories='auto')) for col in categorical_cols]\n",
    ")\n",
    "\n",
    "# Wrap it all in a pipeline.  The parameters of the regression were chosen by \n",
    "# trial and error with GridSearchCV in a separate notebook.\n",
    "pipe = make_pipeline(\n",
    "    mapper, \n",
    "    LogisticRegression(solver='lbfgs', multi_class='ovr',\n",
    "                      max_iter=500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pipe.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = pipe.predict(X_train)\n",
    "accuracy_score(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
