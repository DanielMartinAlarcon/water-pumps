{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('original_data/sample_submission.csv')\n",
    "test_features = pd.read_csv('original_data/test_features.csv')\n",
    "train_features = pd.read_csv('original_data/train_features.csv')\n",
    "train_labels = pd.read_csv('original_data/train_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup1(X):\n",
    "    \"\"\"\n",
    "    cleanup1 gets the data in minimal working order for a logistic regression. I fill\n",
    "    up NANs, change datetime objects to numbers, drop one useless feature and \n",
    "    standardize the datatypes.\n",
    "    \n",
    "    >> Input\n",
    "    X: Original, full-featured dataset (train_features or test_features)\n",
    "    \n",
    "    >> Output\n",
    "    X2: Cleaned dataset ready for logistic regression\n",
    "    \"\"\"\n",
    "    \n",
    "    # Looking at all the features with missing values, it looks like those\n",
    "    # features are all categorical variables where 'unknown' would be a\n",
    "    # category we can work with.  I'll replace the NANs accordingly.\n",
    "    X2 = X.fillna('unknown')\n",
    "    \n",
    "    # Regression on dates won't work.  Instead, I'll turn the \n",
    "    # date_recorded column into the number of years since 2000\n",
    "    # (the earliest date in the training date is from ~2002, and the\n",
    "    # latest from 2013.)\n",
    "    dates = pd.to_datetime(X2.date_recorded)\n",
    "    year2000 = pd.to_datetime('2000-01-01')\n",
    "    years = [i.days/365 for i in (dates - year2000)]\n",
    "    X2.date_recorded = years\n",
    "    \n",
    "    # region_code and district_code are int64, but they should really be\n",
    "    # treated as categories (and there's only 20-30 classes in each).\n",
    "    # I'll cast them as strings instead.\n",
    "    X2.region_code = X2.region_code.astype('str')\n",
    "    X2.district_code = X2.district_code.astype('str')\n",
    "    \n",
    "    # recorded_by has only one value everywhere, and is therefore useless\n",
    "    X2 = X2.drop(columns='recorded_by')\n",
    "    \n",
    "    # To prevent data conversion warnings, I'll turn all the numerical\n",
    "    # features (except id) into float64.\n",
    "    \n",
    "    # Also, some columns contained bool values and NANs.  \n",
    "    # (e.g., public_meeting, permit)\n",
    "    # I replaced the NANs with strings, which created a problem for later\n",
    "    # operations that don't like heterogeneous datatypes within a single\n",
    "    # column. I'll prevent this problem by casting those two features as str.\n",
    "    \n",
    "    type_dict = {'amount_tsh':'float64',\n",
    "                 'date_recorded':'float64',\n",
    "                 'gps_height':'float64',\n",
    "                 'longitude':'float64',\n",
    "                 'latitude':'float64',\n",
    "                 'num_private':'float64',\n",
    "                 'population':'float64',\n",
    "                 'construction_year':'float64',\n",
    "                 'public_meeting':'str',\n",
    "                 'permit':'str'}\n",
    "    \n",
    "    X2 = X2.astype(dtype = type_dict)\n",
    "    \n",
    "    return X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train is the matrix of features that will go into the logistic regression.\n",
    "# It exists at various points as a dataframe or numpy array\n",
    "X_train = cleanup1(train_features)\n",
    "y_train = train_labels['status_group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['source_class',\n",
       " 'permit',\n",
       " 'public_meeting',\n",
       " 'management_group',\n",
       " 'quantity_group',\n",
       " 'quantity',\n",
       " 'quality_group',\n",
       " 'waterpoint_type_group',\n",
       " 'source_type',\n",
       " 'payment',\n",
       " 'payment_type',\n",
       " 'waterpoint_type']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This command produces a series of the categorical features, calculates their cardinality\n",
    "# (number of unique values), sorts the features by cardinality, extracts the feature names\n",
    "# (indices), turns those indexes into a list, and takes all but the 6 with highest cardinality. \n",
    "cols_to_keep = X_train.select_dtypes(exclude='number').nunique().sort_values().index.tolist()[:-6]\n",
    "cols_to_keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = X_train.drop(columns=cols_to_drop)\n",
    "X_train = X_train[cols_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this cell I define a pipeline that will scale and one-hot encode X_train, then\n",
    "# feed it to the logistic regression.\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "\n",
    "# Get lists of numerical and categorical features\n",
    "numerical_cols = X_train.select_dtypes(include='number').columns.tolist()\n",
    "categorical_cols = X_train.select_dtypes(exclude='number').columns.tolist()\n",
    "\n",
    "# Use a mapper to apply different transformations to the numerical and\n",
    "# categorical features\n",
    "mapper = DataFrameMapper(\n",
    "  [([col], RobustScaler()) for col in numerical_cols] +\n",
    "  [([col], OneHotEncoder(categories='auto')) for col in categorical_cols]\n",
    ")\n",
    "\n",
    "# Wrap it all in a pipeline.  The parameters of the regression were chosen by \n",
    "# trial and error with GridSearchCV in a separate notebook.\n",
    "pipe = make_pipeline(\n",
    "    mapper, \n",
    "    LogisticRegression(solver='lbfgs', multi_class='ovr',\n",
    "                      max_iter=500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 20s, sys: 573 ms, total: 1min 21s\n",
      "Wall time: 42.5 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('dataframemapper', DataFrameMapper(default=False, df_out=False,\n",
       "        features=[(['source_class'], OneHotEncoder(categorical_features=None, categories='auto',\n",
       "       dtype=<class 'numpy.float64'>, handle_unknown='error',\n",
       "       n_values=None, sparse=True)), (['permit'], OneHotEncoder(categ...enalty='l2', random_state=None, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "pipe.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.750016835016835"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = pipe.predict(X_train)\n",
    "accuracy_score(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
